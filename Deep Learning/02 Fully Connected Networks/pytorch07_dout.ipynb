{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studying Validation Loss of Test Set With and Without Dropout\n",
    "In this section we see how our model generalizes to never before seen data. What we will discover is that after a certain number of epochs, our model will begin to perform worse on data it hasn't seen before. This is a sign of over-fitting, and is combated through dropout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_palette(sns.color_palette(\"bright\", n_colors=10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Dropout: \n",
    "Let's write a simple network without dropout, train and track its losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return F.log_softmax(self.fc4(x), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier()\n",
    "criterion = nn.NLLLoss()\n",
    "opt = optim.Adam(model.parameters(), lr = 0.003)\n",
    "epochs = 20\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "accuracy = []\n",
    "\n",
    "for e in range(epochs):\n",
    "    test_loss = 0\n",
    "    train_loss = 0\n",
    "    acc = 0\n",
    "    for images, labels in trainloader:\n",
    "        opt.zero_grad()\n",
    "        loss = criterion(model(images), labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "    train_losses.append(train_loss/len(trainloader))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for images, labels in testloader:\n",
    "            logps = model(images)\n",
    "            loss = criterion(logps, labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            ps = torch.exp(logps)\n",
    "            _, top_class = ps.topk(1, dim = 1)\n",
    "            equals = (top_class == labels.view(*top_class.shape)).numpy()\n",
    "            acc += equals.mean()\n",
    "            \n",
    "        accuracy.append(acc/len(testload