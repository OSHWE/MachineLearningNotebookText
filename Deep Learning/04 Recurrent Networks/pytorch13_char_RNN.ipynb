{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory Recurrent Neural Networks in PyTorch\n",
    "Recurrant neural networks of all flavors or designed to mimic the human idea of \"memory\", and are therefore uniquely poised to handle problems that involve sequences of related data. Traditionally, we might think of a sequence as something that spans time (time series data), but we can also apply RNNs to natural language processing problems. \n",
    "\n",
    "## The Idea Behind LSTMs\n",
    "Recurrent neural networks in general maintain state information about the data last passed through the network. This is true of both vanilla RNNs and LSTMs. This \"hidden state\", as it is called is passed back into the network along with each new element of a sequence. Therefore, each output of the network is a function not only of the input variables, but of the hidden state that serves as \"memory\" of what the network has seen in the past.\n",
    "\n",
    "To understand LSTMS, we must understand the gap that they fill in the abilities of traditional RNNs. Where vanilla RNN's fail in that they suffer from rapid gradient vanishing or explosion of gradients. Roughly speaking, this is caused by the fact that the formulation of the hidden state results in an exponential term $W^T$ when applying the chain rule. Therefore, when certain conditions of the base matrix $W$ are met, the term may either explode or vanish very quickly.\n",
    "\n",
    "LSTMs do not suffer (as badly) from this problem, and are therefore able to maintain longer \"memory\", making them ideal for learning temporal data.\n",
    "\n",
    "## Understanding Data Flow: LSTM Layer\n",
    "Perhaps the single most difficult concept to grasp when learning LSTMs after other types of networks is how the data flows through the layers of the model. \n",
    "\n",
    "It's not magic, but it may seem so:\n",
    "\n",
    "1. An LSTM layer is comprised of a set of $N$ hidden nodes. This value $N$ is assigned by the user when the model object is instantiated. \n",
    "\n",
    "2. When a single sequence $S$ of length $M$ is passed into the network, each individual element $s_i$ of the sequence $S$ is passed through each and every hidden node.\n",
    "\n",
    "3. Each hidden node gives a single output for each input it sees. This results in an overall output from the hidden layer of shape $(M, N)$\n",
    "\n",
    "4. If mini-batches of $B$ sequences are fed to the network, there is an additional dimension added, resulting in an output of shape $(B, M, N)$\n",
    "\n",
    "![title](imgs/Flowchart.png)\n",
    "\n",
    "## Understanding Data Flow: Fully Connected Layer\n",
    "After an LSTM layer (or set of LSTM layers), we typically add a fully connected layer to the network for final output via the `nn.Linear()` class.\n",
    "\n",
    "1. The input size for the final `nn.Linear()` layer will always be equal to the number of hidden nodes in the LSTM layer that precedes it.\n",
    "\n",
    "2. The output of this final fully connected layer will depend on the form of the targets and/or loss function you are using.\n",
    "\n",
    "Point 2 above bears expanding on with a couple of examples with different kinds of data and different desired outcomes. That's what we'll do next.\n",
    "\n",
    "## Understanding Data Flow: Examples\n",
    "We will go over 2 examples:\n",
    "\n",
    "1. Regression\n",
    "2. Classification\n",
    "\n",
    "### Example 1a: Regression Network Architecture\n",
    "Consider some time-series data, perhaps stock prices. Given a the past 7 days worth of stock prices for a particular product, we wish to predict the the 8th day's price. In this case, we wish our output to be a single value. We will evaluate the accuracy of this single value using MSE, so for both prediction and for performance evaluations, we need a single valued output. Therefore, we would define our network architecture something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1    # The number of variables in your sequence data. \n",
    "n_hidden   = 100  # The number of hidden nodes in the LSTM layer.\n",
    "n_layers   = 2    # The total number of LSTM models you wish to stack.\n",
    "out_size   = 1    # The size of the output you desire from your RNN\n",
    "\n",
    "lstm   = nn.LSTM(input_size, n_hidden, n_layers)\n",
    "linear = nn.Linear(n_hidden, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1b: Shaping Data Between Layers\n",
    "\n",
    "I'll let you in on a little secret that a friend of mine once told me:\n",
    "\n",
    "> _These days I have an understanding of it_ [LSTM data flow] _that works if I kind of look away while I'm doing it._\n",
    ">\n",
    ">-- Alec\n",
    "\n",
    "While what he says is true in a sense, I think we can pin down some specifics of how this machine works. \n",
    "\n",
    "1. __The input__ to the LSTM layer must be of shape `(batch_size, sequence_length, number_features)`, where `batch_size` refers to the number of sequences per batch and `number_features` is the number of variables in your time series. \n",
    "\n",
    "2. __The output__ of your LSTM layer will be shaped like `(batch_size, sequence_length, hidden_size)`. Take another look at the flow chart I created above. \n",
    "\n",
    "3. __The input__ of our fully connected `nn.Linear()` layer requires an input size corresponding to the number of hidden nodes in the preceding LSTM layer. 