{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory Recurrent Neural Networks in PyTorch\n",
    "Recurrant neural networks of all flavors or designed to mimic the human idea of \"memory\", and are therefore uniquely poised to handle problems that involve sequences of related data. Traditionally, we might think of a sequence as something that spans time (time series data), but we can also apply RNNs to natural language processing problems. \n",
    "\n",
    "## The Idea Behind LSTMs\n",
    "Recurrent neural networks in general maintain state information about the data last passed through the network. This is true of both vanilla RNNs and LSTMs. This \"hidden state\", as it is called is passed back into the network along with each new element of a sequence. Therefore, each output of the network is a function not only of the input variables, but of the hidden state that serves as \"memory\" of what the network has seen in the past.\n",
    "\n",
    "To understand LSTMS, we must understand the gap that they fill in the abilities of traditional RNNs. Where vanilla RNN's fail in that they suffer from rapid gradient vanishing or explosion of gradients. Roughly speaking, this is caused by the fact that the formulation of the hidden state results in an exponential term $W^T$ when applying the chain rule. Therefore, when certain conditions of the base matrix $W$ are met, the term may either explode or vanish very quickly.\n",
    "\n",
    "LSTMs do not suffer (as badly) from this problem, and are therefore able to maintain longer \"memory\", making them ideal for learning temporal data.\n",
    "\n",
    "## Understanding Data Flow: LSTM Layer\n",
    "Perhaps the single most difficult concept to grasp when learning LSTMs after other types of networks is how the data flows through the layers of the model. \n",
    "\n",
    "It's not magic, but it may seem so:\n",
    "\n",
    "1. An LSTM layer is comprised of a set of $N$ hidden nodes. This value $N$ is assigned by the user when the model object is instantiated. \n",
    "\n",
    "2. When a single sequence $S$ of length $M$ is passed into the network, each individual element $s_i$ of the sequence $S$ is passed through each and every hidden node.\n",
    "\n",
    "3. Each hidden node gives a single output for each input it sees. This results in an overall output from the hidden layer of shape $(M, N)$\n",
    "\n",
    "4. If mini-batches of $B$ sequences are fed to the network, there is an additional dimension added, resulting in an output of shape $(B, M, N)$\n",
    "\n",
    "![title](imgs/Flowchart.png)\n",
    "\n",
    "## Understanding Data Flow: Fully Connected Layer\n",
    "After an LSTM layer (or set of LSTM layers), we typically add a fully connected layer to the network for final output via the `nn.Linear()` class.\n",
    "\n",
    "1. The input size for the final `nn.Linear()` layer will always be equal to the number of hidden nodes in the LSTM layer that precedes it.\n",
    "\n",
    "2. The output of this final fully connected layer will depend on the form of the targets and/or loss function you are using.\n",
    "\n",
    "Point 2 above bears expanding on with a couple of examples with different kinds of data and different desired outcomes. That's what we'll do next.\n",
    "\n",
    "## Understanding Data Flow: Examples\n",
    "We will go over 2 examples:\n",
    "\n",
    "1. Regression\n",
    "2. Classification\n",
    "\n",
    "### Example 1a: Regression Network Architecture\n",
    "Consider some time-series data, perhaps stock prices. Given a the past 7 days worth of stock prices for a particular product, we wish to predict the the 8th day's price. In this case, we wish our output to be a single value. We will evaluate the accuracy of this single value using MSE, so for both prediction and for performance evaluations, we need a single valued output. Therefore, we would define our network architecture something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1    # The number of variables in your sequence data. \n",
    "n_hidden   = 100  # The number of hidden nodes in the LSTM layer.\n",
    "n_layers   = 2    # The total number of LSTM models you wish to stack.\n",
    "out_size   = 1    # The size of the output you desire from your RNN\n",
    "\n",
    "lstm   = nn.LSTM(input_size, n_hidden, n_layers)\n",
    "linear = nn.Linear(n_hidden, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1b: Shaping Data Between Layers\n",
    "\n",
    "I'll let you in on a little secret that a friend of mine once told me:\n",
    "\n",
    "> _These days I have an understanding of it_ [LSTM data flow] _that works if I kind of look away while I'm doing it._\n",
    ">\n",
    ">-- Alec\n",
    "\n",
    "While what he says is true in a sense, I think we can pin down some specifics of how this machine works. \n",
    "\n",
    "1. __The input__ to the LSTM layer must be of shape `(batch_size, sequence_length, number_features)`, where `batch_size` refers to the number of sequences per batch and `number_features` is the number of variables in your time series. \n",
    "\n",
    "2. __The output__ of your LSTM layer will be shaped like `(batch_size, sequence_length, hidden_size)`. Take another look at the flow chart I created above. \n",
    "\n",
    "3. __The input__ of our fully connected `nn.Linear()` layer requires an input size corresponding to the number of hidden nodes in the preceding LSTM layer. Therefore we must reshape our data into the form `(batches, n_hidden)`. \n",
    "\n",
    "__Important note:__ `batches` is not the same as `batch_size` in the sense that they are not the same number. However, the idea is the same in that we are dividing up the output of the LSTM layer into `batches` number of pieces, where each piece is of size `n_hidden`, the number of hidden LSTM nodes. \n",
    "\n",
    "Here is some code that simulates passing input data `x` through the entire network, following the protocol above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1    # The number of variables in your sequence data. \n",
    "n_hidden   = 100  # The number of hidden nodes in the LSTM layer.\n",
    "n_layers   = 2    # The total number of LSTM models you wish to stack.\n",
    "out_size   = 1    # The size of the output you desire from your RNN\n",
    "\n",
    "lstm   = nn.LSTM(input_size, n_hidden, n_layers, batch_first=True)\n",
    "linear = nn.Linear(n_hidden, 1)\n",
    "\n",
    "x = get_batches(data)          # -> input x:    (batch_size, seq_length, num_features)\n",
    "x, hs = lstm(x, hs)            # -> LSTM out:   (batch_size, seq_length, hidden_size)\n",
    "x = x.reshape(-1, hidden_size) # -> Linear in:  (batch_size * seq_length, hidden_size)\n",
    "x = linear(x)                  # -> Linear out: (batch_size * seq_length, out_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that `out_size = 1` because we only wish to know a single value, and that single value will be evaluated using MSE as the metric.\n",
    "\n",
    "### Example 2a: Classification Network Architecture\n",
    "\n",
    "In this example, we want to generate some text. A model is trained on a large body of text, perhaps a book, and then fed a single or sequence of characters. The model will look at each character and predict which character should come next. This time our problem is one of classification rather than regression, and we must alter our architecture accordingly. I created this diagram to sketch the general idea:\n",
    "\n",
    "![title](imgs/Drawing.jpg)\n",
    "\n",
    "Perhaps our model has trained on a text of millions of words made up of 50 unique characters. What this means that when our network gets a single character, we wish to know which of 50 characters comes next. Therefore our network output for a single character will be 50 probabilities corresponding to each of 50 possible next characters. \n",
    "\n",
    "Additionally, we will one-hot encode each character in a string of text, meaning the number of variables (`input_size`) is no longer one as it was before, but rather is the size of the one-hot encoded character vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size  = 50  # representing the one-hot encoded vector size\n",
    "hidden_size = 100 # number of hidden nodes in the LSTM layer\n",
    "n_layers    = 2   # number of LSTM layers\n",
    "output_size = 50  # output of 50 scores for the next character\n",
    "\n",
    "lstm   = nn.LSTM(input_size, hidden_size, n_layers, batch_first=True)\n",
    "linear = nn.Linear(hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2b: Shaping Data Between Layers\n",
    "As far as shaping the data between layers, there isn't much difference. The logic is identical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size  = 50  # representing the one-hot encoded vector size\n",
    "hidden_size = 100 # number of hidden nodes in the LSTM layer\n",
    "n_layers    = 2   # number of LSTM layers\n",
    "output_size = 50  # output of 50 scores for the next character\n",
    "\n",
    "lstm   = nn.LSTM(input_size, hidden_size, n_layers, batch_first=True)\n",
    "linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "x = get_batches(data)          # -> input x:    (batch_size, seq_length, num_features)\n",
    "x, hs = lstm(x, hs)            # -> LSTM out:   (batch_size, seq_length, hidden_size)\n",
    "x = x.reshape(-1, hidden_size) # -> Linear in:  (batch_size * seq_length, hidden_size)\n",
    "x = linear(x)                  # -> Linear out: (batch_size * seq_length, out_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this scenario presents a unique challenge. Because we are dealing with categorical predictions, we will likely want to use cross entropy loss to train our model. In this case it is __so important__ to know your loss function's requirements. For example, take a look at PyTorch's `nn.CrossEntropyLoss()` input requirements (emphasis mine, because let's be honest some documentation needs help):\n",
    "\n",
    "\n",
    "> ___The input___ is expected to contain raw, unnormalized scores for each class.\n",
    "> ___The input___ has to be a Tensor of size either (minibatch, C)...\n",
    ">\n",
    "> This criterion __[Cross Entropy Loss]__ expects a class index in the range [0, C-1] as ___the target___ for each value of a __1D tensor__ of size minibatch.\n",
    "\n",
    "Okay, no offense PyTorch, but that's shite. I'm not sure it's even English. Let me translate:\n",
    "\n",
    "1. The prediction (called input above, even though there are two inputs) should be of shape (minibatch, C) where C is the number of possible classes. In our example `C = 50`. \n",
    "\n",
    "2. The target, which is the second input should be of size (minibatch, 1). In other words the target __should not__ be one-hot encoded. \n",
    "\n",
    "What this means for you is that you will have to shape your training data in two different ways. Inputs `x` will be one-hot encoded but your targets `y` must be label encoded. Further, the one-hot columns of `x` should be indexed in line with the label encoding of `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 26 unique characters\n",
    "alphabet = ['a', 'b', ... , 'z']\n",
    "\n",
    "x = np.array(list('abc')) # inputs\n",
    "y = np.array(list('xyz')) # targets\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False).fit(alphabet)\n",
    "label_encoder  = {ch: i for i, ch in enumerate(alphabet)}\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "x = onehot_encoder.transform(x)\n",
    "y = [label_encoder[ch] for ch in y]\n",
    "\n",
    "input_size  = 50  # representing the one-hot encoded vector size\n",
    "hidden_size = 100 # number of hidden nodes in the LSTM layer\n",
    "n_layers    = 2   # number of LSTM layers\n",
    "output_size = 50  # output of 50 scores for the next character\n",
    "\n",
    "lstm   = nn.LSTM(input_size, hidden_size, n_layers, batch_first=True)\n",
    "linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "x = get_batches(data)          # -> input x:    (batch_size, seq_length, num_features)\n",
    "x, hs = lstm(x, hs)            # -> LSTM out:   (batch_size, seq_length, hidden_size)\n",
    "x = x.reshape(-1, hidden_size) # -> Linear in:  (batch_size * seq_length, hidden_size)\n",
    "x = linear(x)                  # -> Linear out: (batch_size * seq_length, out_size)\n",
    "\n",
    "loss = criterion(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parting Thoughts:\n",
    "LSTMs can be complex in their implementation. Most of this complexity can be eliminated by understanding  Each problem you might wish to solve will likely require a seemingly way of doing business. However, I hope that the above gave you some idea of how the logic and flows. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model:\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, vocab, n_hidden, n_layers, do=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab    = vocab\n",
    "        self.int2char = {i: ch for i, ch in enumerate(vocab)}\n",
    "        self.char2int = {ch: i for i, ch in self.int2char.items()}\n",
    "        self.encoder  = OneHotEncoder(sparse=False).fit(vocab.reshape(-1, 1))\n",
    "        \n",
    "        self.n_hidden = n_hidden \n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(len(vocab), n_hidden, n_layers, batch_first=True, dropout=do)\n",
    "        self.fc   = nn.Linear(n_hidden, len(vocab))\n",
    "        \n",
    "    def forward(self, x, hs=None):\n",
    "        x, hs = self.lstm(x, hs)          # -> (batch_size, seq_len, n_hidden)\n",
    "        x = x.reshape(-1, self.n_hidden)  # -> (batch_size * seq_len, n_hidden)\n",
    "        out = self.fc(x)                  # -> (batch_size * seq_len, vocab_size)\n",
    "        \n",
    "        return out, hs \n",
    "    \n",
    "    def predict(self, char, top_k=None, hs=None):\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.to(device)\n",
    "        self.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x = np.array([char])\n",
    "            x = x.reshape(-1, 1)\n",
    "            x = self.onehot_encode(x)\n",
    "            x = x.reshape(1, 1, -1)\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "            x = x.to(device)\n",
    "\n",
    "            out, hs = self(x, hs)\n",
    "\n",
    "            ps   = F.softmax(out, dim=1).squeeze()\n",
    "            \n",
    "            if top_k is None:\n",
    "                choices = np.arange(len(self.vocab))\n",
    "            else:\n",
    "                ps, choices = ps.topk(top_k)\n",
    "                choices = choices.cpu().numpy()\n",
    "            \n",
    "            ps = ps.cpu().numpy()\n",
    "            \n",
    "            char = np.random.choice(choices, p=ps/ps.sum())\n",
    "            char = self.int2char[char]\n",
    "\n",
    "        return char, hs\n",
    "    \n",
    "    \n",
    "    def sample(self, length, top_k=None, primer='And Victoria sang '):\n",
    "        hs = None\n",
    "        for px in primer:\n",
    "            out, hs = self.predict(px, hs=hs)\n",
    "        \n",
    "        chars = [ch for ch in primer]\n",
    "        for ix in range(length):\n",
    "            char, hs = self.predict(chars[-1], top_k=top_k, hs=hs)\n",
    "            chars.append(char)\n",
    "        \n",
    "        return ''.join(chars)\n",
    "    \n",
    "    \n",
    "    def label_encode(self, data):\n",
    "        return np.array([self.char2int[ch] for ch in data])\n",
    "    \n",
    "    \n",
    "    def label_decode(self, data):\n",
    "        return np.array([self.int2char[i] for i in data])\n",
    "    \n",
    "    \n",
    "    def onehot_encode(self, data):\n",
    "        return self.encoder.transform(data)\n",
    "    \n",
    "    \n",
    "    def onehot_decode(self, data):\n",
    "        return self.encoder.inverse_transform(data)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a batching method:\n",
    "def get_batches(data, n_seq, seq_len):\n",
    "    \"\"\"\n",
    "    Takes data of shape (n_samples, n_features), returns batches\n",
    "    of shape (n_seq, seq_len, n_features)\n",
    "    \"\"\"\n",
    "    n_features = data.shape[1]\n",
    "    n_chars    = n_seq * seq_len\n",
    "    n_batches  = int(np.floor(len(data) / n_chars))\n",
    "    n_keep     = n_batches * n_chars\n",
    "    \n",
    "    inputs  = data[:n_keep]\n",
    "    targets = np.append(data[1:], data[0]).reshape(data.shape)\n",
    "    targets = targets[:n_keep]\n",
    "    \n",
    "    inputs = inputs.reshape(n_seq, -1, n_features)\n",
    "    targets = targets.reshape(n_seq, -1, n_features)\n",
    "    \n",
    "    for i in range(0, inputs.shape[1], seq_len):\n",
    "        x = inputs[:, i: i + seq_len]\n",
    "        y = targets[:, i: i + seq_len]\n",
    "        yield x,y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, batch_size, seq_len, epochs, lr=0.01, clip=5, valid=None):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "    \n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if valid is not None:\n",
    "        data  = model.onehot_encode(data.reshape(-1, 1))\n",
    "        valid = model.onehot_encode(valid.reshape(-1, 1))\n",
    "    else:\n",
    "        data  = model.onehot_encode(data.reshape(-1, 1))\n",
    "\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        hs = None\n",
    "        t_loss = 0\n",
    "        v_loss = 0\n",
    "\n",
    "        for x, y in get_batches(data, batch_size, seq_len):\n",
    "            opt.zero_grad()\n",
    "            x = torch.tensor(x).float()\n",
    "            x = x.to(device)\n",
    "            \n",
    "            out, hs = model(x, hs)\n",
    "            hs = tuple([h.data for h in hs])\n",
    "\n",
    "            # invert one-hot of targets for use by cross-entropy loss function\n",
    "            y = y.reshape(-1, len(model.vocab))\n",
    "            y = model.onehot_decode(y)\n",
    "            y = model.label_encode(y.squeeze())\n",
    "            y = torch.from_numpy(y).long().to(device)\n",
    "\n",
    "            loss = criterion(out, y.squeeze())\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            t_loss += loss.item()\n",
    "            \n",
    "        if valid is not None:\n",
    "            model.eval()\n",
    "            hs = None\n",
    "            with torch.no_grad():\n",
    "                for x, y in get_batches(valid, batch_size, seq_len):\n",
    "\n",
    "                    x = torch.tensor(x).float()\n",
    "                    x = x.to(device)\n",
    "\n",
    "                    # invert one-hot of targets for use by cross-entropy loss function\n",
    "                    y = y.reshape(-1, len(model.vocab))\n",
    "                    y = model.onehot_decode(y)\n",
    "                    y = model.label_encode(y.squeeze())\n",
    "                    y = torch.from_numpy(y).long().to(device)\n",
    "\n",
    "                    out, hs = model(x, hs)\n",
    "                    hs = tuple([h.data for h in hs])\n",
    "\n",
    "                    loss = criterion(out, y.squeeze())\n",
    "                    v_loss += loss.item()\n",
    "\n",
    "                valid_loss.append(np.mean(v_loss))\n",
    "        \n",
    "        train_loss.append(np.mean(t_loss))\n",
    "        \n",
    "        if e % 2 == 0:\n",
    "            print(f'------- Epoch {e} ---------')\n",
    "            print(f'Training Loss: {train_loss[-1]}')\n",
    "            if valid_loss:\n",
    "                print(f'Valid Loss: {valid_loss[-1]}')\n",
    "            \n",
    "    plt.plot(train_loss, label=\"Training\")\n",
    "    plt.plot(valid_loss, label=\"Validation\")\n",
    "    plt.title(\"Loss vs Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1786701,), (198522,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data from file:\n",
    "with open('data/texts/anna.txt') as dat