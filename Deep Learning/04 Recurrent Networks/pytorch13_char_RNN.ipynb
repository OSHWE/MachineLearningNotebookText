{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory Recurrent Neural Networks in PyTorch\n",
    "Recurrant neural networks of all flavors or designed to mimic the human idea of \"memory\", and are therefore uniquely poised to handle problems that involve sequences of related data. Traditionally, we might think of a sequence as something that spans time (time series data), but we can also apply RNNs to natural language processing problems. \n",
    "\n",
    "## The Idea Behind LSTMs\n",
    "Recurrent neural networks in general maintain state information about the data last passed through the network. This is true of both vanilla RNNs and LSTMs. This \"hidden state\", as it is called is passed back into the network along with each new element of a sequence. Therefore, each output of the network is a function not only of the input variables, but of the hidden state that serves as \"memory\" of what the network has seen in the past.\n",
    "\n",
    "To understand LSTMS, we must understand the gap that they fill in the abilities of traditional RNNs. Where vanilla RNN's fail in that they suffer from rapid gradient vanishing or explosion of gradients. Roughly speaking, this is caused by the fact that the formulation of the hidden state results in an exponential term $W^T$ when applying the chain rule. Therefore, when certain conditions of the base matrix $W$ ar