{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Precursor - Important Statistics Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Value \n",
    "### Discreet Case:  \n",
    "The probability _weighted_ average of _all possible_ values of a random variable. It is important to note that the word _average_ or the word _mean_ may refer to the average or mean of a sample of the random variable, while expected value refers to the average of all possible outcomes. Therefore any sample mean is an estimate of the expected value. \n",
    "\n",
    "$$E[X] = \\sum_{i=1}^N x_ip_i = \\mu$$\n",
    "\n",
    "Where $x_i$ is the outcome and $p_i$ is the probability of that outcome.\n",
    "\n",
    "_example:_ Let X be the outcome of a fair coin toss. Let X=1 represent landing on heads and $X=2$ represent landing on tails. Then the expected value of $X$ is \n",
    "\n",
    "$$E[X] = \\sum_{i=1}^2 p_ix_i = 0.5\\times 1 + 0.5 \\times 2 = .75$$\n",
    "\n",
    "_note_: If all outcomes are equally likely, as in the above example, then the expected value is simply the arithmetic mean of the possible outcomes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous case:\n",
    "in this case, the expected value is in integral form:\n",
    "\n",
    "$$E[X]=\\int_{\\mathbb{R}} x f(x) dx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where f(x) is that variables probability density function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put some code here to demonstrate calculating expected value of continuous variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Deviation__ \n",
    "The difference between the observed value of a variable and the some other value, such as Expected Value.\n",
    "- If the deviation is between an observed value and the expected value, it is called __error__.\n",
    "- If the deviation is between an observed value and the estimate of the expected value (sample mean), it is called a __residual__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for computing deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Variance__ ($\\sigma^2$) \n",
    "The variance of a random variable $X$ is the expectation of the squared deviation of a random variable from its mean. It is a measure of how spread out from the mean a set of samples is. If $\\mu$ is the mean: \n",
    "\n",
    "$$\\text{Var}(X) = E[(X - \\mu)^2] = \\sum p_i (x_i - \\mu)^2 = \\sigma^2$$\n",
    "\n",
    "Or in the case of N equally likely outcomes: \n",
    "\n",
    "$$\\text{Var}(X) = E[(X - \\mu)^2] = \\frac{1}{N}\\sum (x_i - \\mu)^2 = \\sigma^2$$\n",
    "\n",
    "_note:_ Note that if we are computing sample variance rather than population variance, we would correct for bias using $N-1$. This is due to the fact, that in general we can expect variance to be an understimate when applied to samples instead of the entire populaton. To account for this, we divide by $N-1$ to shift the resulting variance to the right, correcting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to compute variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Standard Deviation__ ($\\sigma$, s)\n",
    "A measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that values tend to be close to the mean, while high standard deviation indicates that values are more spread out. The standard deviation of a variable is the square root of its Variance.  Mathematically it is the square root of __variance__, defined above:\n",
    "\n",
    "$$\\sigma = \\sqrt{\\text{Var}(X)}$$\n",
    "\n",
    "A useful property of standard deviation is the fact that it is expressed in the same units as the data from which it is derived. This allows for human readability and interpretation of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to compute standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Covariance: ($\\sigma_{X,Y}$)__ \n",
    "Covariance is the measure of joint variability of two random variables. For example, if small values of variable $X$ are typically associated with small values of variable $Y$, while large values of $X$ are associated with large values of $Y$, then covariance is positive. \n",
    "\n",
    "$$\\sigma_{X,Y} = E[(X - \\mu_x)(Y - \\mu_y)] = p_i \\sum_{i-=1}^n (x_i - \\mu_x)(y_i - \\mu_y)$$\n",
    "\n",
    "Or if all $n$ outcomes are equally likely:\n",
    "\n",
    "$$\\sigma_{X,Y} = E[(X - \\mu_x)(Y - \\mu_y)] = \\frac{1}{n} \\sum (x_i - \\mu_x)(y_i - \\mu_y)$$\n",
    "\n",
    "Reminder, expected value is the probability weighted mean of the entire population or sample space:\n",
    "\n",
    "$$\\mu_x = E[X] \\\\ \\mu_y = E[Y]$$\n",
    "\n",
    "While the sign (+/-) of covariance is useful in telling us about tendancies in the linear relationships in our data. The magnitude, however is not so useful, as it depends on the magnitude of the variables, which might be vastly different. To interpret the exact nature of the relationship, we must think about the correlation coefficient, in the next definition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Correlation coefficient: ($\\rho_{X,Y}$, r)__ \n",
    "Correlation is a measure of the relatedness between two variables ranging in value from -1 to 1.\n",
    "\n",
    "__$\\rho = 1$__ implies that a straight line can be drawn that passes through ALL data points, implying perfect positive correlation.\n",
    "\n",
    "$0 < \\rho < 1$ implies we see _positive_ correlation between variables, but not all points fall in a stright line. \n",
    "\n",
    "$\\rho =0$ implies that data points are evenly distributed across the sample space (i.e. highly scattered) and no relationship can be found between them. \n",
    "\n",
    "$-1 < \\rho < 0$ implies we see _negative_ correlation between variables, but not all points fall in a stright line. \n",
    "The formula is: \n",
    "\n",
    "$\\rho = -1$ implies that we see perfect _negative_ correlation, much like the case $\\rho = 1$, but this time the slope of the line passing through the data points is negative. \n",
    "\n",
    "$$\\rho_{X,Y} = \\frac{cov(X,Y)}{\\sigma_X \\sigma_Y}\\\\\n",
    "\\sigma_X := \\text{standard deviation of X}\\\\\n",
    "\\sigma_Y := \\text{standard deviation of Y}$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
