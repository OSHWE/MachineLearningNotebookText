{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification: Logistic Regression\n",
    "\n",
    "## Part 1: A brief mathematical overview for those who are interested:\n",
    "__note :__ _I want to note that getting lost in algebra (or calculus or statistics) is not the point of this. I aim to give a simple mathematical overview of what is going on behind the curtain of logistic regression. The math is of interest to me, and perhaps others, but need not be of particular interest for you, so long as you get \"the gist\" of what is going on._\n",
    "\n",
    "Logistic regression is a type of linear model. The basis of that model is the \"odds-function\". If you have done any gambling, this function may be familiar to you. If you want to know the odds of winning (or losing), you find it with this ratio:\n",
    "\n",
    "### $$\\text{odds} = \\frac{p}{1-p}$$\n",
    "\n",
    "For example, if the probability of winning is .75, then your odds of winning are $\\frac{.75}{1-.75} = \\frac{.75}{.25} = \\frac{3}{1}$, or in plain English \"3 to 1\". Conversely, your odds of losing are $\\frac{.25}{.75} = \\frac{1}{3}$, or \"3 to 1 against\". \n",
    "\n",
    "Now, where does the term \"logistic\" come into play, and why did I call this a \"linear\" model? Well let's answer the first question by introducing the \"log-odds\" aka \"logit\" function, which is simply the logarithm of the odds function:\n",
    "\n",
    "$$\\ln {\\frac{p}{1-p}} $$\n",
    "\n",
    "There is a lot of history theory behind the purpose of this mapping of probabilities in the range $[0, 1]$ to all real number $(-\\infty, \\infty)$. A good start is the Wikipedia article [found here.](https://en.wikipedia.org/wiki/Logit) For the purposes of machine learning, however, we now address why this model is considered a \"linear\" one. \n",
    "\n",
    "Recall that in linear regression, our model looks like this:\n",
    "\n",
    "### $$Y = \\beta_0 +  x\\beta_1$$\n",
    "\n",
    "Where $Y$ is the variable we would like to predict, and $x \\dots x_n$ are our predictive variables. This model looks a lot like the equation of a line $y = mx + b$, and that is because it is! We can use fancy Greek letters if we would like (it feels good), but it's all the same. $y$ is the dependent variable, $\\beta_0$ is the \"$y$-intercept\" and $\\beta$ is just slope.\n",
    "\n",
    "The goal of training our linear regression model is to find a \"line of best fit\" through our continuous data points, that allows us to predict an output given an input. One way that this is often done is by finding the values of $\\beta_i$ that minimizing the average of squared differences between our guess for $Y$ and its actual value:\n",
    "\n",
    "### $$\\min_{\\beta_0, \\beta_1} \\frac{1}{n}\\sum_{i=1}^n (y_i - f(x_i))^2$$\n",
    "\n",
    "But why am I talking so much about linear regression when the big words at the top of the page say __logistic regression__? Well, because in logistic regression, we assume a linear relationship between the predictive variables $x \\dots x_n$ and the log-odds function for the event of response variable $Y=1$:\n",
    "\n",
    "### $$\\ln {\\frac{p}{1-p}} = \\beta_0 +  x\\beta_1$$\n",
    "\n",
    "We can then rearrange to solve for our probability in terms of our independent variables and our $\\beta$ parameters. This calculation is shown on Wikipedia's logistic regression page, but here are a few more steps for those who want to know but are algebraically challenged like myself:\n",
    "\n",
    "### $$\\begin{align}\n",
    "\\frac{p}{1-p} &= e^{\\beta_0 + x\\beta_1} \\\\\n",
    "p &= e^{\\beta_0 + x\\beta_1} - p(e^{\\beta_0 + x\\beta_1}) \\\\\n",
    "p + p(e^{\\beta_0 + x\\beta_1}) &= e^{\\beta_0 + x\\beta_1} \\\\\n",
    "p &= \\frac{e^{\\beta_0 + x\\beta_1}}{1 +e^{\\beta_0 + x\\beta_1}} \\\\\n",
    "&= \\frac{1}{1 + e^{-(\\beta_0 + x\\beta_1)}}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, enough algebra, back to logistic regression. We have a binary choice for the value of random variable $Y$ like \"Winning\" or \"Losing\". Let's say that the probability of $Y = \\text{\"winning\"}$ is $p$. Our goal is to train a model that again selects $\\beta_0$ and $\\beta_1$ so that the error of our model is minimized. The [loss function is different](https://towardsdatascience.com/optimization-loss-function-under-the-hood-part-ii-d20a239cde11), but the goal is the same as with linear regression. \n",
    "\n",
    "It is important to note that the goal of logistic regression is first to estimate a probability, __NOT__ an outcome. Let's say, given some input variable $x = \\text{whatever}$, our model predicts that the probability of $Y=\\text{\"winning\"}$ is 0.89. Then, we may logically assign the classification of \"winning\" as our best guess. Our equation doesn't predict \"winning\" or \"losing\", only the probability of \"winning\". We then algorithmically apply the classification of \"winning\" or \"losing\" based on the estimated probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that's enough math for one day. Next up: learning to use linear regression on the Iris data-set.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
