{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification: Logistic Regression\n",
    "\n",
    "## Part 1: A brief mathematical overview for those who are interested:\n",
    "__note :__ _I want to note that getting lost in algebra (or calculus or statistics) is not the point of this. I aim to give a simple mathematical overview of what is going on behind the curtain of logistic regression. The math is of interest to me, and perhaps others, but need not be of particular interest for you, so long as you get \"the gist\" of what is going on._\n",
    "\n",
    "Logistic regression is a type of linear model. The basis of that model is the \"odds-function\". If you have done any gambling, this function may be familiar to you. If you want to know the odds of winning (or losing), you find it with this ratio:\n",
    "\n",
    "### $$\\text{odds} = \\frac{p}{1-p}$$\n",
    "\n",
    "For example, if the probability of winning is .75, then your odds of winning are $\\frac{.75}{1-.75} = \\frac{.75}{.25} = \\frac{3}{1}$, or in plain English \"3 to 1\". Conversely, your odds of losing are $\\frac{.25}{.75} = \\frac{1}{3}$, or \"3 to 1 against\". \n",
    "\n",
    "Now, where does the term \"logistic\" come into play, and why did I call this a \"linear\" model? Well let's answer the first question by introducing the \"log-odds\" aka \"logit\" function, which is simply the logarithm of the odds function:\n",
    "\n",
    "$$\\ln {\\frac{p}{1-p}} $$\n",
    "\n",
    "There is a lot of history theory behind the purpose of this mapping of probabilities in the range $[0, 1]$ to all real number $(-\\infty, \\infty)$. A good start is the Wikipedia article [found here.](https://en.wikipedia.org/wiki/Logit) For the purposes of machine learning, however, we now address why this model is considered a \"linear\" one. \n",
    "\n",
    "Recall that in linear regression, our model looks like this:\n",
    "\n",
    "### $$Y = \\beta_0 +  x\\beta_1$$\n",
    "\n",
    "Where $Y$ is the variable we would like to predict, and $x \\dots x_n$ are our predictive variables. This model looks a lot like the equation of a line $y = mx + b$, and that is because it is! We can use fancy Greek letters if we would like (it feels good), but it's all the same. $y$ is the dependent variable, $\\beta_0$ is the \"$y$-intercept\" and $\\beta$ is just slope.\n",
    "\n",
    "The goal of training our linear regression model is to find a \"line of best fit\" through our continuous data points, that allows us to predict an output given an input. One way that this is often done is by finding the values of $\\beta_i$ that minimizing the average of squared differences between our guess for $Y$ and its actual value:\n",
    "\n",
    "### $$\\min_{\\beta_0, \\beta_1} \\frac{1}{n}\\sum_{i=1}^n (y_i - f(x_i))^2$$\n",
    "\n",
    "But why am I talking so much about linear regression when the big words at the top of the page say __logistic regression__? Well, because in logistic regression, we assume a linear relationship between the predictive variables $x \\dots x_n$ and the log-odds function for the event of response variable $Y=1$:\n",
    "\n",
    "### $$\\ln {\\frac{p}{1-p}} = \\beta_0 +  x\\beta_1$$\n",
    "\n",
    "We can then rearrange to solve for our probability in terms of our independent variables and our $\\beta$ parameters. This calculation is shown on Wikipedia's logistic regression page, but here are a few more steps for those who want to know but are algebraically challenged like myself:\n",
    "\n",
    "### $$\\begin{align}\n",
    "\\frac{p}{1-p} &= e^{\\beta_0 + x\\beta_1} \\\\\n",
    "p &= e^{\\beta_0 + x\\beta_1} - p(e^{\\beta_0 + x\\beta_1}) \\\\\n",
    "p + p(e^{\\beta_0 + x\\beta_1}) &= e^{\\beta_0 + x\\beta_1} \\\\\n",
    "p &= \\frac{e^{\\beta_0 + x\\beta_1}}{1 +e^{\\beta_0 + x\\beta_1}} \\\\\n",
    "&= \\frac{1}{1 + e^{-(\\beta_0 + x\\beta_1)}}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, enough algebra, back to logistic regression. We have a bi